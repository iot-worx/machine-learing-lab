{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3610jvsc74a57bd00f268b1d80e8d4edc8525177b039447c1e5d21a52d27b7375ff6dab2d357ef20",
   "display_name": "Python 3.6.10 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Predictive Maintenance of Turbofan Engines\n",
    "\n",
    "## SageMaker MXNet Estimator\n",
    "\n",
    "First we'll import our variables from the pervious notebook\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('shared_vars', 'rb') as f:\n",
    "    bucket, prefix, s3_train_data = pickle.load(f)"
   ]
  },
  {
   "source": [
    "### MXNet Model Training Script (Out of scope)\n",
    "\n",
    "Training MXNet models using MXNet Estimators is a two-step process. First, you prepare your training script, then second, you run this on SageMaker via an MXNet Estimator. The training script we have prepared for the model is located in the entry_point folder.\n",
    "\n",
    "The training script contains functions to create the model for training and for inference. We also have functions to convert our dataframes into a Gluon Dataset so that it can be efficiently prefetched, transformed into numerical features used by the network and padded so that we can learn from multiple samples in batches.\n",
    "\n",
    "For more information on how to setup a training script for SageMaker using the MXNet estimator see: https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#preparing-the-mxnet-training-script\n",
    "\n",
    "**Important note**\n",
    "\n",
    "** The upper bound for the RUL is set to 130 in the training script, this means that the predictions from the model will be a fraction of this value **\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import boto3\n",
    "from time import strftime, gmtime\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "source": [
    "We import the training data that we previously saved to CSV."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = []\n",
    "\n",
    "for i in range(0,4):\n",
    "    df = pd.read_csv('data/train-{:01d}.csv'.format(i), index_col=0)\n",
    "    train_df.append(df)"
   ]
  },
  {
   "source": [
    "### Train MXNet Estimator\n",
    "\n",
    "We now start the Sagemaker training job by creating an MXNet estimator. We pass some arguments to the MXNet estimator constructor such as `entry_point`, `instance_count` and `instance_type`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pred-maintenance-mxnet-model\"\n",
    "training_job_name = \"{}-{}\".format(model_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "train_instance_type = 'ml.p3.2xlarge'\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "m = MXNet(entry_point='script.py',\n",
    "          source_dir='entry_point',\n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          instance_count=1, \n",
    "          instance_type=train_instance_type,\n",
    "          output_path=output_location,\n",
    "          hyperparameters={'num-datasets' : len(train_df),\n",
    "                           'num-gpus': 1,\n",
    "                           'epochs': 500,\n",
    "                           'optimizer': 'adam',\n",
    "                           'batch-size':1,\n",
    "                           'log-interval': 100},\n",
    "         input_mode='File',\n",
    "         use_spot_instances = True,\n",
    "         max_run = 3600,\n",
    "         max_wait = 3600,\n",
    "         framework_version='1.6.0')"
   ]
  },
  {
   "source": [
    "We kick off the trianing job by calling the fit method. fit has a required argument of the S3 training data location, and we also pass an optional job_name argument which we will use later to call the model for batch transformation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit({'train': s3_train_data}, job_name=training_job_name)"
   ]
  },
  {
   "source": [
    "## Deploy the model\n",
    "\n",
    "\n",
    "### Create Transformer Model\n",
    "We now call the transformer function which will take the training model and create a SageMaker model suitable for deployment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = 's3://{}/{}/{}'.format(bucket, prefix, 'batch-inference')\n",
    "transformer = m.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)"
   ]
  },
  {
   "source": [
    "### Batch transform of test data using the transformer model\n",
    "\n",
    "Using the `transformer` model that we just created we can run a SageMaker Batch Transformation job to get some predictions on the test data sets that we have.\n",
    "\n",
    "Below is a function that copies some test data to a new location in S3 where it's then used as the input for the `transform` fucntion for the SageMaker Batch Transformation Job."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_test_key = '{}/data/test-0.csv'.format(prefix)\n",
    "s3_transform_input = os.path.join(prefix,  \"batch-transform-input\")\n",
    "\n",
    "def get_transform_input():\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_response = s3_client.get_object(Bucket=bucket, Key=s3_test_key)\n",
    "    test_file = s3_response[\"Body\"].read()\n",
    "\n",
    "    test_df_entry = pd.read_csv(io.BytesIO(test_file))\n",
    "    test_data = test_df_entry[test_df_entry['id']==0+1][test_df_entry.columns[2:-1]].values\n",
    "    test_data = test_data[0:test_data.shape[0]-1,:].astype('float32')\n",
    "    data_payload = {'input':np.expand_dims(test_data, axis=0).tolist()}\n",
    "    \n",
    "    job_name = 'predictive-maintenance-batch-transform-job-{}'.format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "    s3_batch_transform_input_key = os.path.join(s3_transform_input, job_name)\n",
    "    \n",
    "    s3_client.put_object(Body=json.dumps(data_payload),\n",
    "                         Bucket=bucket, \n",
    "                         Key=s3_batch_transform_input_key)\n",
    "    return job_name, 's3://{}/{}'.format(bucket, s3_batch_transform_input_key)\n",
    "\n",
    "job_name, input_key = get_transform_input()\n",
    "transformer.transform(input_key, wait=True)"
   ]
  },
  {
   "source": [
    "### View prediction results\n",
    "\n",
    "Once the SageMaker Batch Transform job completes we can see the prediction of the fractional remaining useful life for the sensor readings we provided."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_output():\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_response = s3_client.get_object(Bucket=bucket, Key=os.path.join(prefix, \n",
    "                                                                       'batch-inference', \n",
    "                                                                       job_name+'.out'))\n",
    "    transform_out = np.array(eval(s3_response[\"Body\"].read()))\n",
    "    return transform_out\n",
    "    \n",
    "transform_output = get_transform_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transform_output)"
   ]
  },
  {
   "source": [
    "To see the actual number of predicted cycles until failure we simply multiply the output by our upper bound that was set in the traiing script, 130 in this case."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transform_output * 130)"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3610jvsc74a57bd00f268b1d80e8d4edc8525177b039447c1e5d21a52d27b7375ff6dab2d357ef20",
   "display_name": "Python 3.6.10 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Predictive Maintenance of Turbofan Engines\n",
    "\n",
    "## Process Data\n",
    "\n",
    "The next step is prepare our training and test datasets for training our model.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Training Data\n",
    "\n",
    "We import our data as before except we'll save each of the dataframes into a single array of dataframes.\n",
    "\n",
    "We then need to do normalization of the data so that all our data is on the same scale. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "index_names = ['id', 'cycle']\n",
    "setting_names = ['setting1', 'setting2', 'setting3']\n",
    "sensor_names = ['s{}'.format(i) for i in range(1,22)] \n",
    "columns = index_names + setting_names + sensor_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    eps = 0.000001\n",
    "    df[columns[2:]]=(df[columns[2:]]-df[columns[2:]].min()+eps)/(df[columns[2:]].max()-df[columns[2:]].min()+eps)\n",
    "    return df\n",
    "\n",
    "def calculate_rul(df):\n",
    "    max_cycle_df = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    max_cycle_df.columns = ['id', 'max_cycle']\n",
    "    df = df.merge(max_cycle_df, on=['id'], how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['cycle']\n",
    "    df.drop('max_cycle', axis=1, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = []\n",
    "\n",
    "for i in range(1,5):\n",
    "    df = pd.read_csv('data/train_FD{:03d}.txt'.format(i), delimiter='\\s+', header=None, names=columns)\n",
    "    normalize(df)\n",
    "    train_df.append(df)\n",
    "\n",
    "for i, df in enumerate(train_df):\n",
    "    df = calculate_rul(df)\n",
    "    train_df[i]=df"
   ]
  },
  {
   "source": [
    "### Test data\n",
    "\n",
    "Next we'll read in the test data combining it with the provided actual RUL values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = []\n",
    "\n",
    "for i in range(1,5):\n",
    "    # Load test data\n",
    "    df = pd.read_csv('data/test_FD{:03d}.txt'.format(i), delimiter='\\s+', header=None, names=columns)\n",
    "\n",
    "    # Load the RUL values\n",
    "    df_rul = pd.read_csv('data/RUL_FD{:03d}.txt'.format(i), delimiter='\\s+', header=None)\n",
    "    df_rul.index += 1\n",
    "\n",
    "    # Merge RUL values with the test data\n",
    "    df = df.merge(df_rul, left_on=df.columns[0], right_index=True, how='left')\n",
    "    df.columns = columns + ['RUL_end']\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    df = df.merge(rul, on=['id'], how='left')\n",
    "    df['RUL'] = df['max'] + df['RUL_end'] - df['cycle']\n",
    "    df.drop(['max','RUL_end'], axis=1, inplace=True)\n",
    "\n",
    "    # Normalize\n",
    "    normalize(df)\n",
    "    test_df.append(df)"
   ]
  },
  {
   "source": [
    "## Save training data to S3\n",
    "\n",
    "**Set the two variables, `bucket` and `prefix` below before continuing.**\n",
    "\n",
    "We'll upload all of our data to our S3 bucket so that the SageMaker training instance can access the training data and the test data from that location.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "bucket = ''\n",
    "prefix = ''\n",
    "data_folder = 'data'\n",
    "\n",
    "s3_bucket_resource = boto3.resource('s3').Bucket(bucket)\n",
    "\n",
    "# Upload raw data files to S3\n",
    "for subdir, dirs, files in os.walk(data_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(subdir, file)\n",
    "        s3_path = os.path.join(prefix, full_path)\n",
    "        s3_bucket_resource.Object(s3_path).upload_file(full_path)\n",
    "\n",
    "# Upload processed test data for inference\n",
    "for i in range(len(test_df)):\n",
    "    local_test_file = 'data/test-{}.csv'.format(i)\n",
    "    test_df[i].to_csv(local_test_file)\n",
    "    s3_test_file = os.path.join(prefix, 'data', 'test-{}.csv'.format(i))\n",
    "    s3_bucket_resource.Object(s3_test_file).upload_file(local_test_file)\n",
    "\n",
    "# Upload processed data for training\n",
    "for i in range(len(train_df)):\n",
    "    local_train_file = 'data/train-{}.csv'.format(i)\n",
    "    train_df[i].to_csv(local_train_file)\n",
    "    s3_train_file = os.path.join(prefix, 'train', 'train-{}.csv'.format(i))\n",
    "    s3_bucket_resource.Object(s3_train_file).upload_file(local_train_file)\n",
    "\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, 'train')\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "source": [
    "The code below will save some of our variables to a file so we can use them in later notebooks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('shared_vars', 'wb') as f:\n",
    "    pickle.dump([bucket, prefix, s3_train_data], f)"
   ]
  }
 ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Maintenance of Turbofan Engines\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "First thing to do is take a look at our data to get a feel for it.\n",
    "\n",
    "We will create a list of the column names using the information from the included `readme.txt` file.\n",
    "\n",
    "* 'unit number' has been substituted for `id` as it represents the unique identifier for an engine\n",
    "* 'time, in cycles' has been shortened to `cycle`\n",
    "* 'operational setting *X*' has been shortened to `settingX`\n",
    "* 'sensor measurement  *X*' has been shottened to `sensorX`\n",
    "\n",
    "*N.B. We can use whatever column names we like but for this workshop these are the column names that the proceding code will be expecting to see*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Column names \n",
    "index_names = ['id', 'cycle']\n",
    "setting_names = ['setting1', 'setting2', 'setting3']\n",
    "sensor_names = ['s{}'.format(i) for i in range(1,22)] \n",
    "columns = index_names + setting_names + sensor_names\n",
    "\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a look at the first training dataset, i.e. `train_FD001.txt`\n",
    "\n",
    "Let's import that to a pandas dataframe as well as adding our column names to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training dataset passing in the list of column names\n",
    "train1 = pd.read_csv('data/train_FD001.txt', delimiter='\\s+', header=None, names=columns)\n",
    "train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous workshop we can take a look at the shape of the data to see how many rows and columns we have.\n",
    "\n",
    "We can use the `.describe()` method to get some basic statistical information from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the width of the screen we can't see the entire output of the describe function so lets flip the output.\n",
    "\n",
    "Using the `.transpose()` method will reflect a dataframe on the diagonal, by writing the rows as columns and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing RUL\n",
    "\n",
    "We'll now compute the target variable, Remaining Useful Life. At this stage will allow us to plot sensor signals against the RUL allowing us to eaily interpret the data.\n",
    "\n",
    "Mathematically we can compute the Remaining Useful Life by `max_cycle - cycle` for each enigne id.\n",
    "* We group the dataframe by `id` into a new dataframe, `max_cycle_df`\n",
    "* Compute the `max_cycle` field \n",
    "* Merge the `max_cycle_df` back into original dataframe\n",
    "* Compute RUL by subtracting `cycle` from `max_cycle`\n",
    "* Drop the `max_cycle` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rul(df):\n",
    "    max_cycle_df = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    max_cycle_df.columns = ['id', 'max_cycle']\n",
    "    df = df.merge(max_cycle_df, on=['id'], how='left')\n",
    "    df['RUL'] = df['max_cycle'] - df['cycle']\n",
    "    df.drop('max_cycle', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train1 = calculate_rul(train1)\n",
    "train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot a histogram that will show us the distribution of the RUL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_max_rul = train1[['id', 'RUL']].groupby('id').max().reset_index()\n",
    "df_max_rul['RUL'].hist(bins=15, figsize=(15,7))\n",
    "plt.xlabel('RUL')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us a couple of interesting things:\n",
    "\n",
    "1. Most engines breakdown around the 200 cycle mark.\n",
    "1. The distribution is right skewed, with few engines lasting over the 300 cycle mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many life to death cycle we have and how do they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##There are 100 engines in first dataset whose RUL is coming to '0'\n",
    "len(train1[train1['RUL'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This confirms why we want to predict them because all of them starting healthy and reaching to RUL '0' at different cycles\n",
    "# because if they would have been collapsing at some fix cycle we don't need to to put all these efforts.\n",
    "one_engine = []\n",
    "for i,r in train1.iterrows():\n",
    "    rul = r['RUL']\n",
    "    one_engine.append(rul)\n",
    "    if rul == 0:\n",
    "        plt.plot(one_engine)\n",
    "        one_engine = []\n",
    "        \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Sensors\n",
    "\n",
    "Next we'll plot the trends for each of the 21 sensors.\n",
    "\n",
    "Due to the size of the dataset, it's not practical to plot every value. Therefor we will only plot engines with an id divisible by 10.\n",
    "\n",
    "We will also reverse the x-axis so that the RUL decreases along the axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensor(df, sensor_name):\n",
    "    plt.figure(figsize=(13,5))\n",
    "    for i in df['id'].unique():\n",
    "        if (i % 10 == 0):  ##\n",
    "            plt.plot('RUL', sensor_name, \n",
    "                     data=df[df['id']==i])\n",
    "    plt.xlim(250, 0)\n",
    "    plt.xticks(np.arange(0, 275, 25))\n",
    "    plt.ylabel(sensor_name)\n",
    "    plt.xlabel('Remaining Use fulLife')\n",
    "    plt.show()\n",
    "\n",
    "for sensor_name in sensor_names:\n",
    "    plot_sensor(train1, sensor_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these graphs we can make a few observations:\n",
    "\n",
    "1. A few of the sensor reading don't vary throughout our dataset. \n",
    "1. Sensor readings tend to stay somewhat consistant before trending up or down towards failure\n",
    "\n",
    "We could assume that using this dataset that we could omit the readings from s1, s5, s10, s16, s18 and s19.\n",
    "\n",
    "However, lets check the other datasets so see if they show the same trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CSV data into dataframes\n",
    "train2 = pd.read_csv('data/train_FD002.txt', delimiter='\\s+', header=None, names=columns)\n",
    "train3 = pd.read_csv('data/train_FD003.txt', delimiter='\\s+', header=None, names=columns)\n",
    "train4 = pd.read_csv('data/train_FD004.txt', delimiter='\\s+', header=None, names=columns)\n",
    "\n",
    "# Calculate the RUL values\n",
    "train2 = calculate_rul(train2)\n",
    "train3 = calculate_rul(train3)\n",
    "train4 = calculate_rul(train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_name in sensor_names:\n",
    "    plot_sensor(train2, sensor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_name in sensor_names:\n",
    "    plot_sensor(train3, sensor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor_name in sensor_names:\n",
    "    plot_sensor(train4, sensor_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have guessed life isn't going to be that easy. We can't discard any of the columns as they could be holding valuable information that we just can't see.\n",
    "\n",
    "In this case we did't have to clean to do any cleaning of the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

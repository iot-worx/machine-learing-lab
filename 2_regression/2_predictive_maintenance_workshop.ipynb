{
 "cells": [
  {
   "source": [
    "# Preditive Maintenance Notebook\n",
    "\n",
    "***Paste the name of your S3 bucket into the 'bucket' variable in the cell below, i.e.,*** \n",
    "\n",
    "bucket = 'my-s3-bucket-name'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import urllib\n",
    "from time import strftime, gmtime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = ''"
   ]
  },
  {
   "source": [
    "## Data Preperation\n",
    "\n",
    "#### Training Data\n",
    "\n",
    "The first step is to gather, store, de-noise, clean and normalize our input data.\n",
    "\n",
    "* We download and unzip the data \n",
    "* We will train our model on each of the 4 training data sets\n",
    "* For each of the training data sets, we'll remove the last 2 columns (NaNs)\n",
    "* We compute the RUL (Max_Cycle - Current_Cycle)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "urllib.request.urlretrieve('https://ti.arc.nasa.gov/m/project/prognostic-repository/CMAPSSData.zip', os.path.join(data_folder, 'CMAPSSData.zip'))\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(data_folder, 'CMAPSSData.zip'), \"r\") as zip_ref:\n",
    "    zip_ref.extractall(data_folder)\n",
    "    \n",
    "columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3','s4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14','s15', 's16', 's17', 's18', 's19', 's20', 's21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize sensor readings\n",
    "train_df = []\n",
    "eps = 0.000001 # for floating point issues during normalization \n",
    "for i in range(1,5):\n",
    "    df = pd.read_csv('data/train_FD{:03d}.txt'.format(i), delimiter=' ', header=None)\n",
    "    df.drop(df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    df.columns = columns\n",
    "    df[columns[2:]]=(df[columns[2:]]-df[columns[2:]].min()+eps)/(df[columns[2:]].max()-df[columns[2:]].min()+eps)\n",
    "    train_df.append(df)\n",
    "\n",
    "# compute RUL (remaining useful life)\n",
    "for i, df in enumerate(train_df):\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    df = df.merge(rul, on=['id'], how='left')\n",
    "    df['RUL'] = df['max'] - df['cycle']\n",
    "    df.drop('max', axis=1, inplace=True)\n",
    "    train_df[i]=df\n",
    "\n",
    "train_df[0].head()\n",
    "o = train_df[0][columns[2:10]][train_df[0]['id'] == 3].plot(subplots=True, sharex=True, figsize=(20,10), title=\"Train: 8 sensors of Engine 1 before failure\")"
   ]
  },
  {
   "source": [
    "### Test data\n",
    "\n",
    "Next we'll read in the test data combining it with the provided actual RUL values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = []\n",
    "for i in range(1,5):\n",
    "    # Load time series\n",
    "    df = pd.read_csv('data/test_FD{:03d}.txt'.format(i), delimiter=' ', header=None)\n",
    "    df.drop(df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    \n",
    "    # Load the RUL values\n",
    "    df_rul = pd.read_csv('data/RUL_FD{:03d}.txt'.format(i), delimiter=' ', header=None)    \n",
    "    df_rul.drop(df_rul.columns[1], axis=1, inplace=True)\n",
    "    df_rul.index += 1\n",
    "    \n",
    "    # Merge RUL and timeseries and compute RUL per timestamp\n",
    "    df = df.merge(df_rul, left_on=df.columns[0], right_index=True, how='left')\n",
    "    df.columns = columns + ['RUL_end']\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    df = df.merge(rul, on=['id'], how='left') # We get the number of cycles per series\n",
    "    df['RUL'] = df['max'] + df['RUL_end'] - df['cycle'] # The RUL is the number of cycles per series + RUL - how many cycles have already ran\n",
    "    df.drop(['max','RUL_end'], axis=1, inplace=True)\n",
    "    \n",
    "    # Normalize\n",
    "    df[columns[2:]]=(df[columns[2:]]-df[columns[2:]].min()+eps)/(df[columns[2:]].max()-df[columns[2:]].min()+eps)\n",
    "    test_df.append(df)"
   ]
  },
  {
   "source": [
    "## SageMaker MXNet Estimator\n",
    "\n",
    "We'll upload all of our data to our S3 bucket so that the SageMaker training instance can access the training data and the test data from that location."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "prefix = 'pred-maintenance-artifacts'\n",
    "\n",
    "s3_bucket_resource = boto3.resource('s3').Bucket(bucket)\n",
    "\n",
    "# Upload raw data files to S3\n",
    "for subdir, dirs, files in os.walk(data_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(subdir, file)\n",
    "        s3_path = os.path.join(prefix, full_path)\n",
    "        s3_bucket_resource.Object(s3_path).upload_file(full_path)\n",
    "\n",
    "# Upload processed test data for inference\n",
    "for i in range(len(test_df)):\n",
    "    local_test_file = 'data/test-{}.csv'.format(i)\n",
    "    test_df[i].to_csv(local_test_file)\n",
    "    s3_test_file = os.path.join(prefix, 'data', 'test-{}.csv'.format(i))\n",
    "    s3_bucket_resource.Object(s3_test_file).upload_file(local_test_file)\n",
    "\n",
    "# Upload processed data for training\n",
    "for i in range(len(train_df)):\n",
    "    local_train_file = 'data/train-{}.csv'.format(i)\n",
    "    train_df[i].to_csv(local_train_file)\n",
    "    s3_train_file = os.path.join(prefix, 'train', 'train-{}.csv'.format(i))\n",
    "    s3_bucket_resource.Object(s3_train_file).upload_file(local_train_file)\n",
    "\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, 'train')\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "source": [
    "We set the training data location as well as the model output location in our S3 bucket.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### MXNet Model Training Script (Out of scope)\n",
    "\n",
    "Training MXNet models using MXNet Estimators is a two-step process. First, you prepare your training script, then second, you run this on SageMaker via an MXNet Estimator. The training script we have prepared for the model is located in the entry_point folder.\n",
    "\n",
    "The training script contains functions to create the model for training and for inference. We also have functions to convert our dataframes into a Gluon Dataset so that it can be efficiently prefetched, transformed into numerical features used by the network and padded so that we can learn from multiple samples in batches.\n",
    "\n",
    "For more information on how to setup a training script for SageMaker using the MXNet estimator see: https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#preparing-the-mxnet-training-script\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Train MXNet Estimator\n",
    "\n",
    "We now start the Sagemaker training job by creating an MXNet estimator. We pass some arguments to the MXNet estimator constructor such as `entry_point`, `instance_count` and `instance_type`.\n",
    "\n",
    "We kick off the trianing job by calling the `fit` method. `fit` has a required argument of the S3 training data location, and we also pass an optional `job_name` argument which we will use later to call the model for batch transformation.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "model_name = \"pred-maintenance-mxnet-model\"\n",
    "training_job_name = \"{}-{}\".format(model_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "train_instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "m = MXNet(entry_point='script.py',\n",
    "          source_dir='entry_point',\n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          instance_count=1, \n",
    "          instance_type=train_instance_type,\n",
    "          output_path=output_location,\n",
    "          hyperparameters={'num-datasets' : len(train_df),\n",
    "                           'num-gpus': 1,\n",
    "                           'epochs': 500,\n",
    "                           'optimizer': 'adam',\n",
    "                           'batch-size':1,\n",
    "                           'log-interval': 100},\n",
    "         input_mode='File',\n",
    "         max_run=7200,\n",
    "         framework_version='1.6.0')\n",
    "\n",
    "m.fit({'train': s3_train_data}, job_name=training_job_name)"
   ]
  },
  {
   "source": [
    "#### Create Transformer Model\n",
    "\n",
    "We now call the `transformer` function which will take the training model and create a SageMaker model suitable for deployment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = 's3://{}/{}/{}'.format(bucket, prefix, 'batch-inference')\n",
    "transformer = m.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)"
   ]
  },
  {
   "source": [
    "#### Batch transform of test data using the transformer model\n",
    "\n",
    "Using the `transformer` model that we just created we can run a SageMaker Batch Transformation job to get some predictions on the test data sets that we have.\n",
    "\n",
    "Below is a function that copies some test data to a new location in S3 where it's then used as the input for the `transform` fucntion for the SageMaker Batch Transformation Job."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_test_key = \"pred-maintenance-artifacts/data/test-0.csv\"\n",
    "s3_transform_input = os.path.join(prefix,  \"batch-transform-input\")\n",
    "\n",
    "def get_transform_input():\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_response = s3_client.get_object(Bucket=bucket, Key=s3_test_key)\n",
    "    test_file = s3_response[\"Body\"].read()\n",
    "\n",
    "    test_df_entry = pd.read_csv(io.BytesIO(test_file))\n",
    "    test_data = test_df_entry[test_df_entry['id']==0+1][test_df_entry.columns[2:-1]].values\n",
    "    test_data = test_data[0:test_data.shape[0]-1,:].astype('float32')\n",
    "    data_payload = {'input':np.expand_dims(test_data, axis=0).tolist()}\n",
    "    \n",
    "    job_name = 'predictive-maintenance-batch-transform-job-{}'.format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "    s3_batch_transform_input_key = os.path.join(s3_transform_input, job_name)\n",
    "    \n",
    "    s3_client.put_object(Body=json.dumps(data_payload),\n",
    "                         Bucket=bucket, \n",
    "                         Key=s3_batch_transform_input_key)\n",
    "    return job_name, 's3://{}/{}'.format(bucket, s3_batch_transform_input_key)\n",
    "\n",
    "job_name, input_key = get_transform_input()\n",
    "transformer.transform(input_key, wait=True)"
   ]
  },
  {
   "source": [
    "#### View prediction results\n",
    "\n",
    "Once the SageMaker Batch Transform job completes we can see the prediction of the fractional remaining useful life for the sensor readings we provided."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_output():\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_response = s3_client.get_object(Bucket=bucket, Key=os.path.join(prefix, \n",
    "                                                                       'batch-inference', \n",
    "                                                                       job_name+'.out'))\n",
    "    transform_out = np.array(eval(s3_response[\"Body\"].read()))\n",
    "    return transform_out\n",
    "    \n",
    "get_transform_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
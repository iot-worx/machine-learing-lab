{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import urllib\n",
    "from time import strftime, gmtime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "urllib.request.urlretrieve('https://ti.arc.nasa.gov/m/project/prognostic-repository/CMAPSSData.zip', os.path.join(data_folder, 'CMAPSSData.zip'))\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(data_folder, 'CMAPSSData.zip'), \"r\") as zip_ref:\n",
    "    zip_ref.extractall(data_folder)\n",
    "    \n",
    "columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3','s4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14','s15', 's16', 's17', 's18', 's19', 's20', 's21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize sensor readings\n",
    "train_df = []\n",
    "eps = 0.000001 # for floating point issues during normalization \n",
    "for i in range(1,5):\n",
    "    df = pd.read_csv('data/train_FD{:03d}.txt'.format(i), delimiter=' ', header=None)\n",
    "    df.drop(df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    df.columns = columns\n",
    "    df[columns[2:]]=(df[columns[2:]]-df[columns[2:]].min()+eps)/(df[columns[2:]].max()-df[columns[2:]].min()+eps)\n",
    "    train_df.append(df)\n",
    "\n",
    "# compute RUL (remaining useful life)\n",
    "for i, df in enumerate(train_df):\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    df = df.merge(rul, on=['id'], how='left')\n",
    "    df['RUL'] = df['max'] - df['cycle']\n",
    "    df.drop('max', axis=1, inplace=True)\n",
    "    train_df[i]=df\n",
    "\n",
    "train_df[0].head()\n",
    "o = train_df[0][columns[2:10]][train_df[0]['id'] == 3].plot(subplots=True, sharex=True, figsize=(20,10), title=\"Train: 8 sensors of Engine 1 before failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = []\n",
    "for i in range(1,5):\n",
    "    # Load time series\n",
    "    df = pd.read_csv('data/test_FD{:03d}.txt'.format(i), delimiter=' ', header=None)\n",
    "    df.drop(df.columns[[26, 27]], axis=1, inplace=True)\n",
    "    \n",
    "    # Load the RUL values\n",
    "    df_rul = pd.read_csv('data/RUL_FD{:03d}.txt'.format(i), delimiter=' ', header=None)    \n",
    "    df_rul.drop(df_rul.columns[1], axis=1, inplace=True)\n",
    "    df_rul.index += 1\n",
    "    \n",
    "    # Merge RUL and timeseries and compute RUL per timestamp\n",
    "    df = df.merge(df_rul, left_on=df.columns[0], right_index=True, how='left')\n",
    "    df.columns = columns + ['RUL_end']\n",
    "    rul = pd.DataFrame(df.groupby('id')['cycle'].max()).reset_index()\n",
    "    rul.columns = ['id', 'max']\n",
    "    df = df.merge(rul, on=['id'], how='left') # We get the number of cycles per series\n",
    "    df['RUL'] = df['max'] + df['RUL_end'] - df['cycle'] # The RUL is the number of cycles per series + RUL - how many cycles have already ran\n",
    "    df.drop(['max','RUL_end'], axis=1, inplace=True)\n",
    "    \n",
    "    # Normalize\n",
    "    df[columns[2:]]=(df[columns[2:]]-df[columns[2:]].min()+eps)/(df[columns[2:]].max()-df[columns[2:]].min()+eps)\n",
    "    test_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "prefix = 'pred-maintenance-artifacts'\n",
    "\n",
    "s3_bucket_resource = boto3.resource('s3').Bucket(bucket)\n",
    "\n",
    "# Upload raw data files to S3\n",
    "for subdir, dirs, files in os.walk(data_folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(subdir, file)\n",
    "        s3_path = os.path.join(prefix, full_path)\n",
    "        s3_bucket_resource.Object(s3_path).upload_file(full_path)\n",
    "\n",
    "# Upload processed test data for inference\n",
    "for i in range(len(test_df)):\n",
    "    local_test_file = 'data/test-{}.csv'.format(i)\n",
    "    test_df[i].to_csv(local_test_file)\n",
    "    s3_test_file = os.path.join(prefix, 'data', 'test-{}.csv'.format(i))\n",
    "    s3_bucket_resource.Object(s3_test_file).upload_file(local_test_file)\n",
    "\n",
    "# Upload processed data for training\n",
    "for i in range(len(train_df)):\n",
    "    local_train_file = 'data/train-{}.csv'.format(i)\n",
    "    train_df[i].to_csv(local_train_file)\n",
    "    s3_train_file = os.path.join(prefix, 'train', 'train-{}.csv'.format(i))\n",
    "    s3_bucket_resource.Object(s3_train_file).upload_file(local_train_file)\n",
    "\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, 'train')\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "model_name = \"pred-maintenance-mxnet-model\"\n",
    "training_job_name = \"{}-{}\".format(model_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "train_instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "m = MXNet(entry_point='entry_point.py',\n",
    "          source_dir='entry_point',\n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          instance_count=1, \n",
    "          instance_type=train_instance_type,\n",
    "          output_path=output_location,\n",
    "          hyperparameters={'num-datasets' : len(train_df),\n",
    "                           'num-gpus': 1,\n",
    "                           'epochs': 500,\n",
    "                           'optimizer': 'adam',\n",
    "                           'batch-size':1,\n",
    "                           'log-interval': 100},\n",
    "         input_mode='File',\n",
    "         max_run=7200,\n",
    "         framework_version='1.6.0')\n",
    "\n",
    "m.fit({'train': s3_train_data}, job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = 's3://{}/{}/{}'.format(bucket, prefix, 'batch-inference')\n",
    "transformer = m.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_test_key = \"pred-maintenance-artifacts/data/test-0.csv\"\n",
    "s3_transform_input = os.path.join(prefix,  \"batch-transform-input\")\n",
    "\n",
    "def get_transform_input():\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_response = s3_client.get_object(Bucket=bucket, Key=s3_test_key)\n",
    "    test_file = s3_response[\"Body\"].read()\n",
    "\n",
    "    test_df_entry = pd.read_csv(io.BytesIO(test_file))\n",
    "    test_data = test_df_entry[test_df_entry['id']==0+1][test_df_entry.columns[2:-1]].values\n",
    "    test_data = test_data[0:test_data.shape[0]-1,:].astype('float32')\n",
    "    data_payload = {'input':np.expand_dims(test_data, axis=0).tolist()}\n",
    "    \n",
    "    job_name = 'predictive-maintenance-batch-transform-job-{}'.format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "    s3_batch_transform_input_key = os.path.join(s3_transform_input, job_name)\n",
    "    \n",
    "    s3_client.put_object(Body=json.dumps(data_payload),\n",
    "                         Bucket=bucket, \n",
    "                         Key=s3_batch_transform_input_key)\n",
    "    return job_name, 's3://{}/{}'.format(bucket, s3_batch_transform_input_key)\n",
    "\n",
    "job_name, input_key = get_transform_input()\n",
    "transformer.transform(input_key, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_output():\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_response = s3_client.get_object(Bucket=bucket, Key=os.path.join(prefix, \n",
    "                                                                       'batch-inference', \n",
    "                                                                       job_name+'.out'))\n",
    "    transform_out = np.array(eval(s3_response[\"Body\"].read()))\n",
    "    return transform_out\n",
    "    \n",
    "get_transform_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
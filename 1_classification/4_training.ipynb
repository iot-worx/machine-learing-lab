{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numeric-bubble",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "Now that we have preproccessed our data we can finally train the ML model using logistic regression.\n",
    "\n"
   ]
  },
  {
   "source": [
    "We import our libraries as before."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "We import the preprocessed data that we created in the previous notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = pd.read_csv('datasets/titanic_processed.csv')\n",
    "\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.shape"
   ]
  },
  {
   "source": [
    "When training an ML model we split our source data into two parts, one for the actual training of model and the second part is used to evaluate our trained model on. We usually use a split of 80:20 for this.\n",
    "\n",
    "The features that we will use to train our model are all the features (columns) except the 'Suvived' feature.\n",
    "\n",
    "We drop the 'Survived' column from the dataframe and save the rest of the features in the 'X' variable.\n",
    "\n",
    "The target variable or label, in our case 'Survived', will be saved to the 'Y' variable.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic_df.drop('Survived', axis=1)\n",
    "Y = titanic_df['Survived']\n"
   ]
  },
  {
   "source": [
    "Scikit-learn provides us a handy method that will spit our data out into training and testing subsets.\n",
    "\n",
    "When called, the `.train_test_split()` method automatically shuffles the data, then splits it up based on a parameter that we provide. As we want an 80:20 split we give it the `test_size=0.2` parameter.\n",
    "\n",
    "The output of calling this method will be saved into four variables:\n",
    "- x_train\n",
    "- x_test\n",
    "- y_train\n",
    "- y_test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "source": [
    "We can now take a look at the shape each of the variables, let's look at the training features and labels first"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "source": [
    "We'll use 569 rows of our dataset to train our model.\n",
    "\n",
    "Now let's look at the test features and labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "source": [
    "The remaining 143 rows will be used to test the accuracy of our model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A logistic regression model can be easily built and trained in scikit-learn using the logistic regression estimator.\n",
    "\n",
    "An estimator in scikit-learn is a high level object that makes it easy to build and train models for prediction.\n",
    "\n",
    "We create an instance of the the estimator we pass in a few parameters to 'design' our model in a certain way.\n",
    "\n",
    "* `penalty='l2'` is a default value used by the estimator, it implies that we are regualrizing our model by applying a penalty on models that are overly complex.\n",
    "    * The 'l2' penalty uses the l2 norm of the coefficients of the model. (l2 norm is the sum of the squares of the coeffcieints)\n",
    "* `C=1.0` specifies the strength of the regularization, 'C' stand for inverse of regularization strength.\n",
    "* `solver='liblinear'` specifies the optimisation algorithm that the estimator will use, 'liblinear' is good for small datasets\n",
    "\n",
    "We then the `.fit()` function on our estimator to perform the training.\n",
    "\n",
    "More info on the LogisticRegression estimator can be found at https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear').fit(x_train, y_train)"
   ]
  },
  {
   "source": [
    "Now that we have trained our model it's time to see how the predictions turn out.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_model.predict(x_test)"
   ]
  },
  {
   "source": [
    "We'll set up the actual 'Survived' values from our data and the predicted values from our models in a dataframe so that we can see them side by side."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = pd.DataFrame({'y_test': y_test,\n",
    "                             'y_pred': y_pred})"
   ]
  },
  {
   "source": [
    "Let's take a sample of actual vs predicted results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results.head()"
   ]
  },
  {
   "source": [
    "We must set up an objective way to measure the performance of our model.\n",
    "\n",
    "We can create a confusion matrix to view the results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_crosstab = pd.crosstab(pred_results.y_pred, pred_results.y_test)\n",
    "\n",
    "titanic_crosstab"
   ]
  },
  {
   "source": [
    "We can see from this that most of the results are in the true-positive and true-negative cells.\n",
    "\n",
    "There are however some false-positive and false-negative values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Scikit-learn prvides us with the tools to properly measure the accuracy of our model.\n",
    "\n",
    "* Accuracy score indicates how many of the predicted values did the model get right. Higher than 50% is better than random guessing for a binary classfier.\n",
    "* Precision score indicates how many passngers the model thought survived actually did survice. 80% shows us there were few false positives.\n",
    "* Recall score indicates how many of the actual survivors did the model correctly predict. This lower score tells us there were many false negatives."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy Score: \", acc)\n",
    "print(\"Pricision Score: \", prec)\n",
    "print(\"Recall Score: \", recall)"
   ]
  },
  {
   "source": [
    "We can calculate these scores manually by breaking down the confusion matrix into true positive, true negative, false positive and false negative then performing some basic mathematical functions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = titanic_crosstab[1][1]\n",
    "TN = titanic_crosstab[0][0]\n",
    "FP = titanic_crosstab[0][1]\n",
    "FN = titanic_crosstab[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_verified = (TP + TN) / (TP + FP + TN + FN)\n",
    "accuracy_score_verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score_verified = TP / (TP + FP)\n",
    "precision_score_verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score_verified = TP / (TP + FN)\n",
    "recall_score_verified"
   ]
  },
  {
   "source": [
    "As we can see the scores we calculated manually are exactly the same as the scores given to us by the scikit-learn function, never hurts to double check to be sure."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3613jvsc74a57bd05fe95147f33012273cbe36a4af9b64c2d7c6194ae48cc8f141788b2e9b581426",
   "display_name": "Python 3.6.13 64-bit ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}